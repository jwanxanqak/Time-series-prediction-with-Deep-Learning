{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "passing-means",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/recurrent-neural-networks-rnns-and-time-series-forecasting-d9ea933426b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-liechtenstein",
   "metadata": {},
   "source": [
    "# Fundamentos de RNN y LSTM con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee597a",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prepared-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "import matplotlib.pyplot as plt                    \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474fdf4d",
   "metadata": {},
   "source": [
    "### generate_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "governing-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports the numpy module and renames it np for easier access.\n",
    "import numpy as np\n",
    "\n",
    "#defines a function called generate_time_series that takes two arguments: batch_size and n_steps\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    \n",
    "    #generates four arrays of random numbers between 0 and 1. Each array has a form of (batch_size, 1)\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    \n",
    "    #generates a sequence of n_steps numbers equally spaced between 0 and 1\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    \n",
    "    #generates a time series based on a sine function. The frequency and phase of the sine function are determined by \n",
    "    #freq1 and offsets1, respectively.\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))    # +wave 1\n",
    "    \n",
    "    #add another time series based on a sine function to the existing series. The frequency and phase of this sine \n",
    "    #function are determined by freq2 and offsets2, respectively\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))            \n",
    "    \n",
    "    #adds random noise to the time series. The noise is an array of random numbers between -0.5 and 0.5.\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)    \n",
    "    \n",
    "    #devuelve la serie de tiempo con una dimensión adicional y convierte los datos a float32\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intimate-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a variable n_steps which is set to 50. This variable will be used to determine the number of time steps \n",
    "#in each time series.\n",
    "n_steps = 50\n",
    "\n",
    "#call the generate_time_series function you defined above, generating 10,000 time series, each with n_steps + 1 time steps.\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "\n",
    "#divides the time series into training sets. X_train contains the first n_steps of the first 7000 series, and y_train \n",
    "#contains the last time step of the same 7000 series.\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "\n",
    "#divides time series into validation sets. X_valid contains the first n_steps of the series 7000 to 9000, and y_valid \n",
    "#contains the last time step of the same series.\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "\n",
    "#divides time series into test sets. X_test contains the first n_steps of series 9000 onwards, and y_test contains \n",
    "#the last time step of the same series\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-hearing",
   "metadata": {},
   "source": [
    "## RNN Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19409b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "governing-station",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - loss: 0.0423 - val_loss: 0.0286\n",
      "Epoch 2/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0246 - val_loss: 0.0184\n",
      "Epoch 3/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0162 - val_loss: 0.0137\n",
      "Epoch 4/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0128 - val_loss: 0.0121\n",
      "Epoch 5/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0113 - val_loss: 0.0117\n",
      "Epoch 6/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0115 - val_loss: 0.0116\n",
      "Epoch 7/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 8/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0114 - val_loss: 0.0116\n",
      "Epoch 9/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 10/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 11/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 12/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 13/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0109 - val_loss: 0.0116\n",
      "Epoch 14/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0112 - val_loss: 0.0116\n",
      "Epoch 15/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 16/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0110 - val_loss: 0.0116\n",
      "Epoch 17/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 18/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0113 - val_loss: 0.0116\n",
      "Epoch 19/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0111 - val_loss: 0.0116\n",
      "Epoch 20/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0110 - val_loss: 0.0116\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.011577751487493515"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Definición del model\n",
    "model = keras.models.Sequential([\n",
    "    #Añadimos una capa de RNN con una neurona\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "\n",
    "# Definimos el optimizador, en este caso de Adam \n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# Generamos el modelo\n",
    "model.compile(loss=\"mse\", \n",
    "              optimizer=optimizer)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Evaluación del modelo\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-authorization",
   "metadata": {},
   "source": [
    "## RNN profunda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "uniform-caution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 29ms/step - loss: 0.2032 - val_loss: 0.0233\n",
      "Epoch 2/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - loss: 0.0201 - val_loss: 0.0133\n",
      "Epoch 3/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - loss: 0.0111 - val_loss: 0.0084\n",
      "Epoch 4/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0076 - val_loss: 0.0062\n",
      "Epoch 5/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 0.0061 - val_loss: 0.0051\n",
      "Epoch 6/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - loss: 0.0052 - val_loss: 0.0046\n",
      "Epoch 7/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 8/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 9/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 10/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 0.0040 - val_loss: 0.0037\n",
      "Epoch 11/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 0.0038 - val_loss: 0.0038\n",
      "Epoch 12/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 13/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 14/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 15/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 16/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step - loss: 0.0034 - val_loss: 0.0035\n",
      "Epoch 17/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 47ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 18/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - loss: 0.0033 - val_loss: 0.0035\n",
      "Epoch 19/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 33ms/step - loss: 0.0033 - val_loss: 0.0037\n",
      "Epoch 20/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - loss: 0.0032 - val_loss: 0.0036\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0036206489894539118"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos un modelo de RNN con múltiples capas\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(10, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(10, return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-taiwan",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "advised-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un modelo básico de LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.fit_generator(generator,epochs=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-savannah",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
